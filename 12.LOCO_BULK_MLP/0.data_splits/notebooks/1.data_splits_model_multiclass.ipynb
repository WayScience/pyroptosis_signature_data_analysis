{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "8e60628c",
            "metadata": {
                "papermill": {
                    "duration": 0.002385,
                    "end_time": "2023-10-18T18:20:04.868387",
                    "exception": false,
                    "start_time": "2023-10-18T18:20:04.866002",
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "## Hyperparameter tuning via Optuna"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8fac71a5",
            "metadata": {
                "papermill": {
                    "duration": 0.001919,
                    "end_time": "2023-10-18T18:20:04.873048",
                    "exception": false,
                    "start_time": "2023-10-18T18:20:04.871129",
                    "status": "completed"
                },
                "tags": []
            },
            "source": [
                "### Being a binary model this notebook will be limited to predicting one class 1 or 0, yes or no.\n",
                "### Here I will be predicting if a cell received a treatment or not"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "860a61dd",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:20:04.878009Z",
                    "iopub.status.busy": "2023-10-18T18:20:04.877582Z",
                    "iopub.status.idle": "2023-10-18T18:20:08.479452Z",
                    "shell.execute_reply": "2023-10-18T18:20:08.478953Z"
                },
                "papermill": {
                    "duration": 3.605361,
                    "end_time": "2023-10-18T18:20:08.480497",
                    "exception": false,
                    "start_time": "2023-10-18T18:20:04.875136",
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "import argparse\n",
                "import json\n",
                "import pathlib\n",
                "import sys\n",
                "\n",
                "import numpy as np\n",
                "import optuna\n",
                "import pandas as pd\n",
                "import pyarrow.parquet as pq\n",
                "import toml\n",
                "import torch\n",
                "from sklearn import preprocessing\n",
                "\n",
                "MLP_parent_path = pathlib.Path(\"../../../utils/\")\n",
                "sys.path.append(str(MLP_parent_path.resolve()))\n",
                "MLP_path = pathlib.Path(\"../../../utils/MLP_utils\").resolve()\n",
                "\n",
                "from MLP_utils.parameters import Parameters\n",
                "from MLP_utils.utils import (\n",
                "    Dataset_formatter,\n",
                "    data_split,\n",
                "    extract_best_trial_params,\n",
                "    objective_model_optimizer,\n",
                "    parameter_set,\n",
                "    plot_metric_vs_epoch,\n",
                "    results_output,\n",
                "    test_optimized_model,\n",
                "    train_optimized_model,\n",
                "    un_nest,\n",
                ")\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "from utils import df_stats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "15fbe814",
            "metadata": {},
            "outputs": [],
            "source": [
                "# set up the parser\n",
                "parser = argparse.ArgumentParser(description=\"Run hyperparameter optimization\")\n",
                "parser.add_argument(\n",
                "    \"--cell_type\",\n",
                "    type=str,\n",
                "    default=\"all\",\n",
                "    help=\"Cell type to run hyperparameter optimization for\",\n",
                ")\n",
                "parser.add_argument(\n",
                "    \"--model_name\",\n",
                "    type=str,\n",
                "    default=\"all\",\n",
                "    help=\"Model name to run hyperparameter optimization for\",\n",
                ")\n",
                "\n",
                "# parse arguments\n",
                "args = parser.parse_args()\n",
                "\n",
                "CELL_TYPE = args.cell_type\n",
                "MODEL_NAME = args.model_name"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "394294ce",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:20:08.498713Z",
                    "iopub.status.busy": "2023-10-18T18:20:08.498501Z",
                    "iopub.status.idle": "2023-10-18T18:20:08.502638Z",
                    "shell.execute_reply": "2023-10-18T18:20:08.502299Z"
                },
                "papermill": {
                    "duration": 0.007175,
                    "end_time": "2023-10-18T18:20:08.503377",
                    "exception": false,
                    "start_time": "2023-10-18T18:20:08.496202",
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "ml_configs_file = pathlib.Path(MLP_path / \"multi_class_config.toml\").resolve(\n",
                "    strict=True\n",
                ")\n",
                "ml_configs = toml.load(ml_configs_file)\n",
                "params = Parameters()\n",
                "mlp_params = parameter_set(params, ml_configs)\n",
                "\n",
                "# overwrite params via command line arguments from papermill\n",
                "mlp_params.CELL_TYPE = CELL_TYPE\n",
                "mlp_params.MODEL_NAME = MODEL_NAME\n",
                "MODEL_TYPE = mlp_params.MODEL_TYPE\n",
                "HYPERPARAMETER_BATCH_SIZE = mlp_params.HYPERPARAMETER_BATCH_SIZE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "44baa945",
            "metadata": {
                "execution": {
                    "iopub.execute_input": "2023-10-18T18:20:08.507805Z",
                    "iopub.status.busy": "2023-10-18T18:20:08.507608Z"
                },
                "papermill": {
                    "duration": 54.581971,
                    "end_time": "2023-10-18T18:21:03.087265",
                    "exception": false,
                    "start_time": "2023-10-18T18:20:08.505294",
                    "status": "completed"
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Import Data\n",
                "# set data file path under pathlib path for multi-system use\n",
                "\n",
                "file_path = pathlib.Path(\n",
                "    f\"../../../data/{mlp_params.CELL_TYPE}_preprocessed_sc_norm_aggregated.parquet\"\n",
                ").resolve(strict=True)\n",
                "\n",
                "df1 = pd.read_parquet(file_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "e8ef96f3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get paths for toml files\n",
                "ground_truth_file_path = pathlib.Path(MLP_path / \"ground_truth.toml\").resolve(\n",
                "    strict=True\n",
                ")\n",
                "treatment_splits_file_path = pathlib.Path(MLP_path / \"splits.toml\").resolve(strict=True)\n",
                "# read toml files\n",
                "ground_truth = toml.load(ground_truth_file_path)\n",
                "treatment_splits = toml.load(treatment_splits_file_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "29c1f3cb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get information from toml files\n",
                "apoptosis_groups_list = ground_truth[\"Apoptosis\"][\"apoptosis_groups_list\"]\n",
                "pyroptosis_groups_list = ground_truth[\"Pyroptosis\"][\"pyroptosis_groups_list\"]\n",
                "healthy_groups_list = ground_truth[\"Healthy\"][\"healthy_groups_list\"]\n",
                "test_split_100 = treatment_splits[\"splits\"][\"data_splits_100\"]\n",
                "test_split_75 = treatment_splits[\"splits\"][\"data_splits_75\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "159e2281",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data Subset Is Off\n"
                    ]
                }
            ],
            "source": [
                "np.random.seed(0)\n",
                "if mlp_params.DATA_SUBSET_OPTION == \"True\":\n",
                "    df1 = df1.groupby(\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\").apply(\n",
                "        lambda x: x.sample(n=mlp_params.DATA_SUBSET_NUMBER, random_state=0)\n",
                "    )\n",
                "    print(\"Data Subset Is On\")\n",
                "    print(f\"Data is subset to {mlp_params.DATA_SUBSET_NUMBER} per treatment group\")\n",
                "    print(df1.shape)\n",
                "    df1.reset_index(drop=True, inplace=True)\n",
                "else:\n",
                "    print(\"Data Subset Is Off\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "6a6ec3fa",
            "metadata": {},
            "outputs": [],
            "source": [
                "# add apoptosis, pyroptosis and healthy columns to dataframe\n",
                "df1[\"apoptosis\"] = df1[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"].isin(\n",
                "    apoptosis_groups_list\n",
                ")\n",
                "df1[\"pyroptosis\"] = df1[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"].isin(\n",
                "    pyroptosis_groups_list\n",
                ")\n",
                "df1[\"healthy\"] = df1[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"].isin(\n",
                "    healthy_groups_list\n",
                ")\n",
                "\n",
                "# merge apoptosis, pyroptosis, and healthy columns into one column\n",
                "conditions = [\n",
                "    (df1[\"apoptosis\"] == True),\n",
                "    (df1[\"pyroptosis\"] == True),\n",
                "    (df1[\"healthy\"] == True),\n",
                "]\n",
                "choices = [\"apoptosis\", \"pyroptosis\", \"healthy\"]\n",
                "df1[\"labels\"] = np.select(conditions, choices, default=\"healthy\")\n",
                "\n",
                "# drop apoptosis, pyroptosis, and healthy columns\n",
                "df1.drop(columns=[\"apoptosis\", \"pyroptosis\", \"healthy\"], inplace=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3cb179dd",
            "metadata": {},
            "source": [
                "### Split said data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "20568fd5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Wells held out for testing: ['B19' 'B20' 'C16' 'C19' 'D16' 'D18' 'E13' 'E14' 'E16' 'E23' 'F13' 'F20'\n",
                        " 'F23' 'G17' 'G23' 'H15' 'H19' 'H20' 'I14' 'I21' 'I22' 'J18' 'J21' 'K15'\n",
                        " 'K20' 'K23' 'L17' 'L23' 'M14' 'M16' 'M19' 'N15' 'N17' 'N23' 'O13' 'O17'\n",
                        " 'O22']\n",
                        "Wells to use for training, validation, and testing ['B13' 'B14' 'B15' 'B16' 'B17' 'B18' 'B19' 'B20' 'B21' 'B22' 'B23' 'C13'\n",
                        " 'C14' 'C15' 'C16' 'C17' 'C18' 'C19' 'C20' 'C21' 'C22' 'C23' 'D13' 'D14'\n",
                        " 'D15' 'D16' 'D17' 'D18' 'D19' 'D20' 'D21' 'D22' 'D23' 'E13' 'E14' 'E15'\n",
                        " 'E16' 'E17' 'E18' 'E19' 'E20' 'E21' 'E22' 'E23' 'F13' 'F14' 'F15' 'F16'\n",
                        " 'F17' 'F18' 'F19' 'F20' 'F21' 'F22' 'F23' 'G13' 'G14' 'G15' 'G16' 'G17'\n",
                        " 'G18' 'G19' 'G20' 'G21' 'G22' 'G23' 'H13' 'H14' 'H15' 'H16' 'H17' 'H18'\n",
                        " 'H19' 'H20' 'H21' 'H22' 'H23' 'I13' 'I14' 'I15' 'I16' 'I17' 'I18' 'I19'\n",
                        " 'I20' 'I21' 'I22' 'I23' 'J13' 'J14' 'J15' 'J16' 'J17' 'J18' 'J19' 'J20'\n",
                        " 'J21' 'J22' 'J23' 'K13' 'K14' 'K15' 'K16' 'K17' 'K18' 'K19' 'K20' 'K21'\n",
                        " 'K22' 'K23' 'L13' 'L14' 'L15' 'L16' 'L17' 'L18' 'L19' 'L20' 'L21' 'L22'\n",
                        " 'L23' 'M13' 'M14' 'M15' 'M16' 'M17' 'M18' 'M19' 'M20' 'M21' 'M22' 'M23'\n",
                        " 'N13' 'N14' 'N15' 'N16' 'N17' 'N18' 'N19' 'N20' 'N21' 'N22' 'N23' 'O13'\n",
                        " 'O14' 'O15' 'O16' 'O17' 'O18' 'O19' 'O20' 'O21' 'O22' 'O23']\n",
                        "(37, 1230) (117, 1230)\n"
                    ]
                }
            ],
            "source": [
                "# randomly select wells to hold out for testing one per treatment group\n",
                "# stratified by treatment group\n",
                "np.random.seed(seed=0)\n",
                "wells_to_hold = (\n",
                "    df1.groupby(\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\")\n",
                "    .agg(np.random.choice)[\"Metadata_Well\"]\n",
                "    .to_list()\n",
                ")\n",
                "df_holdout = df1[df1[\"Metadata_Well\"].isin(wells_to_hold)]\n",
                "df = df1[~df1[\"Metadata_Well\"].isin(wells_to_hold)]\n",
                "\n",
                "\n",
                "print(\"Wells held out for testing:\", df_holdout[\"Metadata_Well\"].unique())\n",
                "print(\n",
                "    \"Wells to use for training, validation, and testing\", df1[\"Metadata_Well\"].unique()\n",
                ")\n",
                "print(df_holdout.shape, df.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "5a46d3a7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(15, 1230) (102, 1230)\n"
                    ]
                }
            ],
            "source": [
                "# variable test and train set splits\n",
                "# 100% test set\n",
                "# subset the following treatments for test set\n",
                "treatment_holdout = df[\n",
                "    df[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"].isin(test_split_100)\n",
                "]\n",
                "df = df[~df[\"oneb_Metadata_Treatment_Dose_Inhibitor_Dose\"].isin(test_split_100)]\n",
                "print(treatment_holdout.shape, df.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "9bf407c4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(81, 1230) (21, 1230)\n",
                        "\n",
                        "    Testing set length: 21\n",
                        "\n",
                        "    Training set length: 64\n",
                        "\n",
                        "    Validation set length: 17\n",
                        "\n",
                        "    Treatment Holdout set length: 15\n",
                        "\n",
                        "    Holdout set length: 37\n",
                        "    Added set length: 154\n",
                        "    Total actual set length: 154\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "training_data_set, testing_data_set = train_test_split(\n",
                "    df,\n",
                "    test_size=0.20,\n",
                "    stratify=df[\"labels\"],\n",
                ")\n",
                "\n",
                "print(training_data_set.shape, testing_data_set.shape)\n",
                "\n",
                "training_data_set, val_data_set = train_test_split(\n",
                "    training_data_set,\n",
                "    test_size=0.20,\n",
                "    stratify=training_data_set[\"labels\"],\n",
                ")\n",
                "print(\n",
                "    f\"\"\"\n",
                "    Testing set length: {len(testing_data_set)}\\n\n",
                "    Training set length: {len(training_data_set)}\\n\n",
                "    Validation set length: {len(val_data_set)}\\n\n",
                "    Treatment Holdout set length: {len(treatment_holdout)}\\n\n",
                "    Holdout set length: {len(df_holdout)}\n",
                "    Added set length: {len(testing_data_set) + len(training_data_set) + len(val_data_set) + len(treatment_holdout) + len(df_holdout)}\n",
                "    Total actual set length: {len(df1)}\n",
                "\"\"\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "5da1578c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# get the indexes for the training and testing sets\n",
                "\n",
                "training_data_set_index = training_data_set.index\n",
                "val_data_set_index = val_data_set.index\n",
                "testing_data_set_index = testing_data_set.index\n",
                "treatment_holdout_index = treatment_holdout.index\n",
                "df_holdout_index = df_holdout.index\n",
                "\n",
                "assert len(training_data_set_index) + len(val_data_set_index) + len(\n",
                "    testing_data_set_index\n",
                ") + len(treatment_holdout_index) + len(df_holdout_index) == len(df1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "bce398ed",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(64,) (17,) (21,) (15,) (37,)\n",
                        "154\n"
                    ]
                }
            ],
            "source": [
                "print(\n",
                "    training_data_set_index.shape,\n",
                "    val_data_set_index.shape,\n",
                "    testing_data_set_index.shape,\n",
                "    treatment_holdout_index.shape,\n",
                "    df_holdout_index.shape,\n",
                ")\n",
                "print(\n",
                "    training_data_set_index.shape[0]\n",
                "    + val_data_set_index.shape[0]\n",
                "    + testing_data_set_index.shape[0]\n",
                "    + treatment_holdout_index.shape[0]\n",
                "    + df_holdout_index.shape[0]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "ad856f53",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>labeled_data_index</th>\n",
                            "      <th>label</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>21</td>\n",
                            "      <td>train</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>38</td>\n",
                            "      <td>train</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>94</td>\n",
                            "      <td>train</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>75</td>\n",
                            "      <td>train</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>148</td>\n",
                            "      <td>train</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>149</th>\n",
                            "      <td>136</td>\n",
                            "      <td>holdout</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>150</th>\n",
                            "      <td>142</td>\n",
                            "      <td>holdout</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>151</th>\n",
                            "      <td>143</td>\n",
                            "      <td>holdout</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>152</th>\n",
                            "      <td>147</td>\n",
                            "      <td>holdout</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>153</th>\n",
                            "      <td>152</td>\n",
                            "      <td>holdout</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>154 rows \u00d7 2 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "     labeled_data_index    label\n",
                            "0                    21    train\n",
                            "1                    38    train\n",
                            "2                    94    train\n",
                            "3                    75    train\n",
                            "4                   148    train\n",
                            "..                  ...      ...\n",
                            "149                 136  holdout\n",
                            "150                 142  holdout\n",
                            "151                 143  holdout\n",
                            "152                 147  holdout\n",
                            "153                 152  holdout\n",
                            "\n",
                            "[154 rows x 2 columns]"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# create pandas dataframe with all indexes and their respective labels, stratified by phenotypic class\n",
                "index_data = []\n",
                "for index in training_data_set_index:\n",
                "    index_data.append({\"labeled_data_index\": index, \"label\": \"train\"})\n",
                "for index in val_data_set_index:\n",
                "    index_data.append({\"labeled_data_index\": index, \"label\": \"val\"})\n",
                "for index in testing_data_set_index:\n",
                "    index_data.append({\"labeled_data_index\": index, \"label\": \"test\"})\n",
                "for index in treatment_holdout_index:\n",
                "    index_data.append({\"labeled_data_index\": index, \"label\": \"treatment_holdout\"})\n",
                "for index in df_holdout_index:\n",
                "    index_data.append({\"labeled_data_index\": index, \"label\": \"holdout\"})\n",
                "\n",
                "# make index data a dataframe and sort it by labeled data index\n",
                "index_data = pd.DataFrame(index_data)\n",
                "index_data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "70798091",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array(['train', 'val', 'test', 'treatment_holdout', 'holdout'],\n",
                            "      dtype=object)"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "index_data[\"label\"].unique()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "931ee871",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "../indexes/SHSY5Y/multi_class\n"
                    ]
                }
            ],
            "source": [
                "save_path = pathlib.Path(f\"../indexes/{CELL_TYPE}/multi_class/\")\n",
                "\n",
                "print(save_path)\n",
                "# create save path if it doesn't exist\n",
                "save_path.mkdir(parents=True, exist_ok=True)\n",
                "# save indexes as tsv file\n",
                "index_data.to_csv(\n",
                "    f\"{save_path}/{params.CELL_TYPE}_data_split_indexes.tsv\", sep=\"\\t\", index=False\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "75021dd9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['apoptosis' 'healthy' 'pyroptosis'] [ 8 74 72]\n",
                        "['apoptosis', 'healthy', 'pyroptosis'] [0.948051948051948, 0.5194805194805194, 0.5324675324675325]\n"
                    ]
                }
            ],
            "source": [
                "# get the class weights for the loss function to account for class imbalance\n",
                "# get the number of samples in each class\n",
                "targets, counts = np.unique(df1[\"labels\"], return_counts=True)\n",
                "print(targets, counts)\n",
                "total_counts = np.sum(counts)\n",
                "# get the class weights\n",
                "class_weights = []\n",
                "class_targets = []\n",
                "for class_name in enumerate(targets):\n",
                "    class_targets.append(class_name[1])\n",
                "for count in enumerate(counts):\n",
                "    class_weights.append(1 - (count[1] / total_counts))\n",
                "print(class_targets, class_weights)\n",
                "# write the class weights to a file for use in the model\n",
                "class_weights_file = pathlib.Path(f\"../class_weights/{CELL_TYPE}/multi_class/\")\n",
                "class_weights_file.mkdir(parents=True, exist_ok=True)\n",
                "class_targets_dicts = {\n",
                "    class_targets[i]: class_weights[i] for i in range(len(class_targets))\n",
                "}\n",
                "# write the file to json\n",
                "class_weights_file = class_weights_file / \"class_weights.json\"\n",
                "with open(class_weights_file, \"w\") as file:\n",
                "    json.dump(class_targets_dicts, file)"
            ]
        }
    ],
    "metadata": {
        "jupytext": {
            "cell_metadata_filter": "-all",
            "encoding": "# coding: utf-8",
            "executable": "/usr/bin/env python",
            "formats": "ipynb,py",
            "main_language": "python"
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.15"
        },
        "papermill": {
            "default_parameters": {},
            "duration": 59.524284,
            "end_time": "2023-10-18T18:21:03.252265",
            "environment_variables": {},
            "exception": null,
            "input_path": "Hyperparameter_Optimization_model_binary.ipynb",
            "output_path": "Hyperparameter_Optimization_model_binary.ipynb",
            "parameters": {
                "CELL_TYPE": "PBMC",
                "CONTROL_NAME": "DMSO_0.100_%_DMSO_0.025_%",
                "MODEL_NAME": "DMSO_0.025_vs_Thapsigargin_10",
                "TREATMENT_NAME": "Thapsigargin_10.000_uM_DMSO_0.025_%"
            },
            "start_time": "2023-10-18T18:20:03.727981",
            "version": "2.4.0"
        },
        "vscode": {
            "interpreter": {
                "hash": "72ae02083a9ca7d143c492d1aec380c7bf553ec51bd66e90e72bba65228121b6"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
